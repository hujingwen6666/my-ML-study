{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#统计出现的单词\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建文档向量，表明输入文档中有没有词汇表中的某个词\n",
    "def words2vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:\n",
    "            print(word,\"is not in my Vocabulary!\")\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建词袋模型，在文档向量的基础上记录了每个词出现的次数\n",
    "def words2vecBag(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else:\n",
    "            print(word,\"is not in my Vocabulary!\")\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please', 'take', 'I', 'dog', 'ate', 'mr', 'maybe', 'him', 'steak', 'not', 'stop', 'stupid', 'dalmation', 'so', 'quit', 'park', 'posting', 'buying', 'has', 'my', 'to', 'problems', 'garbage', 'food', 'licks', 'help', 'cute', 'is', 'flea', 'worthless', 'how', 'love']\n"
     ]
    }
   ],
   "source": [
    "sentenceList,sentenceClass = loadDataSet()\n",
    "myVocabulary = createVocabList(sentenceList)\n",
    "print(myVocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(words2vecBag(myVocabulary,sentenceList[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#vecList训练集中每一个样本生成的向量\n",
    "#样本的类别列表\n",
    "def trainNB(samplesVec,classList):\n",
    "    numSample = len(samplesVec)\n",
    "    numWords = len(classList)\n",
    "    numClass = len(set(classList))\n",
    "    #计算属于每个类别的概率\n",
    "    classP = {}\n",
    "    for aclass in classList:\n",
    "        classP[str(aclass)] = classP.get(str(aclass),0) + 1\n",
    "    for key in classP:\n",
    "        #为了不出现0，上面加λ，下面加Sλ\n",
    "        classP[key] = (classP[key]+1)/float(numWords+numClass*1)\n",
    "    #计算以每个class为条件的每个w的条件概率\n",
    "    classWP={}\n",
    "    for key in classP:\n",
    "        #初始值都设为1，避免后期概率相乘时出现0\n",
    "        classWP[key]=np.ones(len(samplesVec[0]))\n",
    "    for i in range(numSample):\n",
    "        classlabel = str(classList[i])\n",
    "        classWP[classlabel] = classWP[classlabel]+samplesVec[i]\n",
    "    \n",
    "    for key in classWP:\n",
    "        classWP[key] = np.log(classWP[key]/np.sum(classWP[key]))\n",
    "        #classWP[key] = classWP[key]/(np.sum(classWP[key])-numWords+numClass)\n",
    "    return classP,classWP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceList,sentenceClass = loadDataSet()\n",
    "myVocabulary = createVocabList(sentenceList)\n",
    "trainMat=[]\n",
    "for item in sentenceList:\n",
    "    trainMat.append(words2vec(myVocabulary,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classP,classWP = trainNB(trainMat,sentenceClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0.5, '1': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': array([-3.33220451, -4.02535169, -3.33220451, -3.33220451, -3.33220451,\n",
       "        -3.33220451, -4.02535169, -2.9267394 , -3.33220451, -4.02535169,\n",
       "        -3.33220451, -4.02535169, -3.33220451, -3.33220451, -4.02535169,\n",
       "        -4.02535169, -4.02535169, -4.02535169, -3.33220451, -2.63905733,\n",
       "        -3.33220451, -3.33220451, -4.02535169, -4.02535169, -3.33220451,\n",
       "        -3.33220451, -3.33220451, -3.33220451, -3.33220451, -4.02535169,\n",
       "        -3.33220451, -3.33220451]),\n",
       " '1': array([-3.93182563, -3.23867845, -3.93182563, -2.83321334, -3.93182563,\n",
       "        -3.93182563, -3.23867845, -3.23867845, -3.93182563, -3.23867845,\n",
       "        -3.23867845, -2.54553127, -3.93182563, -3.93182563, -3.23867845,\n",
       "        -3.23867845, -3.23867845, -3.23867845, -3.93182563, -3.93182563,\n",
       "        -3.23867845, -3.93182563, -3.23867845, -3.23867845, -3.93182563,\n",
       "        -3.93182563, -3.93182563, -3.93182563, -3.93182563, -2.83321334,\n",
       "        -3.93182563, -3.93182563])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classWP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(sampleVec,classP,classWP):\n",
    "    classResult={}\n",
    "    for key in classWP:\n",
    "        temp = np.sum(sampleVec*classWP[key]) + np.log(classP[key])\n",
    "        classResult[key] = temp\n",
    "    sortedclassResult = sorted(classResult.items(),key = lambda d:d[1],reverse = True)\n",
    "    return sortedclassResult[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEntry = ['love','my','dalmation']\n",
    "classifyNB(words2vec(myVocabulary,testEntry),classP,classWP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEntry = ['stupid','garbage']\n",
    "classifyNB(words2vec(myVocabulary,testEntry),classP,classWP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,random\n",
    "from os.path import join\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for filename in os.listdir('spam'):\n",
    "        with open(join('spam',filename),errors='ignore')as fp:\n",
    "            wordList = textParse(fp.read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "    for filename in os.listdir('ham'):\n",
    "        with open(join('ham',filename),errors='ignore')as fp:\n",
    "            wordList = textParse(fp.read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    #create vocabulary\n",
    "    vocabList = createVocabList(docList)\n",
    "    \n",
    "    trainingSet = list(range(50)); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    \n",
    "    trainMat=[]; trainClasses = []\n",
    "    \n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB\n",
    "        trainMat.append(words2vecBag(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    classP,classWP = trainNB(trainMat,trainClasses)\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = words2vecBag(vocabList, docList[docIndex])\n",
    "        if classifyNB(wordVector,classP,classWP) != str(classList[docIndex]):\n",
    "            errorCount += 1\n",
    "            print (\"classification error:\\n\",docList[docIndex])\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error:\n",
      " ['oem', 'adobe', 'microsoft', 'softwares', 'fast', 'order', 'and', 'download', 'microsoft', 'office', 'professional', 'plus', '2007', '2010', '129', 'microsoft', 'windows', 'ultimate', '119', 'adobe', 'photoshop', 'cs5', 'extended', 'adobe', 'acrobat', 'pro', 'extended', 'windows', 'professional', 'thousand', 'more', 'titles']\n",
      "classification error:\n",
      " ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "the error rate is:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
